{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Ln1CcX0sN8"
      },
      "outputs": [],
      "source": [
        "#Q1 scatter plot on iris\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "iris_data = pd.read_csv(\"/content/iris.csv\")\n",
        "print(iris_data.head())\n",
        "x = iris_data['sepal.length']\n",
        "y = iris_data['sepal.width']\n",
        "species = iris_data['variety']\n",
        "plt.figure(figsize=(10, 6))\n",
        "for sp in species.unique():\n",
        "    sp_data = iris_data[species == sp]\n",
        "    plt.scatter(sp_data['sepal.length'], sp_data['sepal.width'], label=sp)\n",
        "plt.title('Scatter Plot of Iris Dataset')\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2 find null values and replace with mean\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "iris_data = pd.read_csv(\"/content/iris.csv\")\n",
        "print(\"Original Data:\")\n",
        "print(iris_data.head())\n",
        "print(\"\\nNull values in each column before imputation:\")\n",
        "print(iris_data.isnull().sum())\n",
        "\n",
        "total_null_values = iris_data.isnull().sum().sum()\n",
        "print(\"\\nTotal number of null values in the dataset:\", total_null_values)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "iris_data_imputed = pd.DataFrame(imputer.fit_transform(iris_data.iloc[:, :-1]), columns=iris_data.columns[:-1])\n",
        "iris_data_imputed['variety'] = iris_data['variety']\n",
        "print(\"\\nData after imputation:\")\n",
        "print(iris_data_imputed.head())\n",
        "print(\"\\nNull values in each column after imputation:\")\n",
        "print(iris_data_imputed.isnull().sum())\n"
      ],
      "metadata": {
        "id": "bXHlTTmA1TWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3 convert categorical values to numeric format in a given dataset using label encoding and one hot encoder\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "iris_data = pd.read_csv(\"/content/iris.csv\")\n",
        "print(\"Original Data:\")\n",
        "print(iris_data.head())\n",
        "label_encoder = LabelEncoder()\n",
        "iris_data['Species_Label'] = label_encoder.fit_transform(iris_data['variety'])\n",
        "print(\"\\nData with Label Encoded Species:\")\n",
        "print(iris_data.head())\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "species_onehot = onehot_encoder.fit_transform(iris_data[['variety']])\n",
        "species_onehot_df = pd.DataFrame(species_onehot, columns=onehot_encoder.get_feature_names_out(['variety']))\n",
        "iris_data = pd.concat([iris_data, species_onehot_df], axis=1)\n",
        "print(\"\\nData with One-Hot Encoded Species:\")\n",
        "print(iris_data.head())\n"
      ],
      "metadata": {
        "id": "hJ8TzAFz1Vnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4 scale values\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "salary_data = pd.read_csv(\"/content/salary.csv\")\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(salary_data.head())\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "salary_data[['Salary']] = imputer.fit_transform(salary_data[['Salary']])\n",
        "\n",
        "categorical_columns = salary_data.select_dtypes(include=['object']).columns\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    salary_data[col] = label_encoders[col].fit_transform(salary_data[col])\n",
        "\n",
        "features = salary_data.drop('Salary', axis=1)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "scaled_salary_data = pd.concat([scaled_features_df, salary_data['Salary']], axis=1)\n",
        "\n",
        "print(\"\\nScaled Data:\")\n",
        "print(scaled_salary_data.head())\n"
      ],
      "metadata": {
        "id": "10m_sq3O1WzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set B\n",
        "#1 split data into training and test set\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris_data = pd.read_csv(\"/content/iris.csv\")\n",
        "print(\"Original Data:\")\n",
        "print(iris_data.head())\n",
        "X = iris_data.drop('variety', axis=1)\n",
        "y = iris_data['variety']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"\\nTraining set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
        "print(\"\\nTraining set:\")\n",
        "print(X_train.head(), y_train.head())\n",
        "print(\"\\nTest set:\")\n",
        "print(X_test.head(), y_test.head())\n"
      ],
      "metadata": {
        "id": "6gv_IqKB1YCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set B\n",
        "# 2 scale features using standardization\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "data = pd.read_csv(\"/content/salary.csv\")\n",
        "print(\"Original Data:\")\n",
        "print(data.head())\n",
        "features = data.drop('Target', axis=1, errors='ignore')\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "if 'Target' in data.columns:\n",
        "    scaled_data = pd.concat([scaled_features_df, data['Target']], axis=1)\n",
        "else:\n",
        "    scaled_data = scaled_features_df\n",
        "print(\"Scaled Data:\")\n",
        "print(scaled_data.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "64YOhhzA1ZU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQ_bUnUPD7th"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}