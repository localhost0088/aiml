{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-wBwm3b0msY"
      },
      "outputs": [],
      "source": [
        "# k-means algorithm on synthetic data blob module\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def generate_data(n_samples=300, random_state=42):\n",
        "    X, y = make_blobs(n_samples=n_samples, n_features=6, centers=3, random_state=random_state)\n",
        "    scaler = MinMaxScaler(feature_range=(10, 150))\n",
        "    X = scaler.fit_transform(X)\n",
        "    df = pd.DataFrame(X, columns=[\"Annual_Income\", \"Spending_Score\", \"Age\", \"Savings\", \"Debt\", \"Credit_Score\"])\n",
        "    df[\"Customer_Segment\"] = y\n",
        "    return df\n",
        "\n",
        "def apply_kmeans(X, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    return kmeans.labels_, kmeans.cluster_centers_\n",
        "\n",
        "def plot_clusters(X, labels, centers):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
        "    plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
        "    plt.title('Customer Segmentation using K-Means')\n",
        "    plt.xlabel('Annual Income (in $1000s)')\n",
        "    plt.ylabel('Spending Score')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "dataset = generate_data()\n",
        "print(dataset.head())\n",
        "\n",
        "labels, centers = apply_kmeans(dataset.iloc[:, :-1].values)\n",
        "\n",
        "plot_clusters(dataset.iloc[:, [0, 1]].values, labels, centers[:, :2])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agglomerative clustering synthetic data blobs module\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "def generate_data(n_samples=100, random_state=42):\n",
        "    X, y = make_blobs(n_samples=n_samples, n_features=6, centers=3, random_state=random_state)\n",
        "    scaler = MinMaxScaler(feature_range=(10, 15))\n",
        "    X = scaler.fit_transform(X)\n",
        "    df = pd.DataFrame(X, columns=[\"Annual_Income\", \"Spending_Score\", \"Age\", \"Savings\", \"Debt\", \"Credit_Score\"])\n",
        "    df[\"Customer_Segment\"] = y\n",
        "    return df\n",
        "\n",
        "def apply_agglomerative_clustering(X, n_clusters=3):\n",
        "    agglo = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    labels = agglo.fit_predict(X)\n",
        "    return labels\n",
        "\n",
        "def plot_clusters(X, labels):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o', edgecolors='k')\n",
        "    plt.title('Customer Segmentation using Agglomerative Clustering')\n",
        "    plt.xlabel('Annual Income (in $1000s)')\n",
        "    plt.ylabel('Spending Score')\n",
        "    plt.show()\n",
        "\n",
        "def plot_dendrogram(X):\n",
        "    linked = linkage(X, method='ward')\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    dendrogram(linked)\n",
        "    plt.title('Hierarchical Clustering Dendrogram')\n",
        "    plt.xlabel('Data Points')\n",
        "    plt.ylabel('Euclidean Distance')\n",
        "    plt.show()\n",
        "\n",
        "dataset = generate_data()\n",
        "print(dataset.head())\n",
        "\n",
        "labels_agglo = apply_agglomerative_clustering(dataset.iloc[:, :-1].values)\n",
        "plot_clusters(dataset.iloc[:, [0, 1]].values, labels_agglo)\n",
        "\n",
        "plot_dendrogram(dataset.iloc[:, :-1].values)\n"
      ],
      "metadata": {
        "id": "4_3gldpV2130"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal no of clusters using elbow and apply k-means\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "from scipy.stats import mode\n",
        "\n",
        "df = pd.read_csv(\"/content/Mall_Customers.csv\")\n",
        "print(df.head())\n",
        "\n",
        "X = df[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
        "\n",
        "wcss = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()\n",
        "\n",
        "optimal_k = np.argmax(np.diff(wcss, 2)) + 2\n",
        "print(f\"The optimal number of clusters is: {optimal_k}\")\n",
        "\n",
        "optimal_k = 3\n",
        "\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df[\"Cluster\"] = kmeans.fit_predict(X)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "for i in range(optimal_k):\n",
        "    plt.scatter(X[df[\"Cluster\"] == i, 0], X[df[\"Cluster\"] == i, 1], label=f'Cluster {i}')\n",
        "\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
        "plt.xlabel(\"Annual Income (k$)\")\n",
        "plt.ylabel(\"Spending Score (1-100)\")\n",
        "plt.title(\"Customer Segmentation using K-Means\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-lqrH8lE24ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agglomerative clustering and show clusters\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "df = pd.read_csv(\"/content/penguins.csv\")\n",
        "print(df.head())\n",
        "\n",
        "df_filtered = df[['flipper_length_mm', 'body_mass_g']].dropna().copy()\n",
        "X = df_filtered.values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "linked = linkage(X_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linked)\n",
        "plt.title('Dendrogram for Agglomerative Clustering')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "plt.show()\n",
        "\n",
        "threshold = 8\n",
        "\n",
        "clusters = fcluster(linked, threshold, criterion='distance')\n",
        "\n",
        "optimal_clusters = len(np.unique(clusters))\n",
        "print(f\"The optimal number of clusters is: {optimal_clusters}\")\n",
        "\n",
        "agglo = AgglomerativeClustering(n_clusters=optimal_clusters)\n",
        "df_filtered[\"clusters\"] = agglo.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "for cluster in range(optimal_clusters):\n",
        "    plt.scatter(X[df_filtered[\"clusters\"] == cluster, 0],\n",
        "                X[df_filtered[\"clusters\"] == cluster, 1], label=f'Cluster {cluster}')\n",
        "\n",
        "plt.xlabel(\"Flipper Length (mm)\")\n",
        "plt.ylabel(\"Body Mass (g)\")\n",
        "plt.title(\"Penguin Clusters using Agglomerative Clustering\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZaLixHp526gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apriori\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "transactions = [\n",
        "    ['Milk', 'Bread', 'Butter'],\n",
        "    ['Bread', 'Butter'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Eggs'],\n",
        "    ['Milk', 'Bread', 'Diaper', 'Butter'],\n",
        "    ['Bread', 'Butter', 'Diaper'],\n",
        "    ['Milk', 'Bread', 'Diaper', 'Beer'],\n",
        "    ['Bread', 'Butter'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Cola'],\n",
        "    ['Milk', 'Bread', 'Butter'],\n",
        "    ['Bread', 'Diaper', 'Cola']\n",
        "]\n",
        "\n",
        "print(\"Transactions:\\n\", transactions)\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df = pd.DataFrame(te_array, columns=te.columns_)\n",
        "print(\"\\nBinary Matrix of Transactions:\\n\", df.head())\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
        "\n",
        "print(\"\\nFrequent Itemsets:\\n\", frequent_itemsets)\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "print(\"\\nAssociation Rules:\\n\", rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "def predict_next_item(input_items):\n",
        "    input_items = set(input_items)\n",
        "\n",
        "    matched_rules = rules[rules['antecedents'].apply(lambda x: input_items.issubset(x))]\n",
        "\n",
        "    if matched_rules.empty:\n",
        "        print(\"\\nNo strong association found. Try with a different combination of items.\")\n",
        "        return None\n",
        "\n",
        "    best_rule = matched_rules.loc[matched_rules['confidence'].idxmax()]\n",
        "    predicted_item = list(best_rule['consequents'])[0]\n",
        "\n",
        "    print(f\"\\nBased on your input items {input_items}, you are likely to buy '{predicted_item}' with confidence of {best_rule['confidence']:.2f}\")\n",
        "    return predicted_item\n",
        "\n",
        "input_items = input(\"Enter the items you bought (comma separated): \").split(',')\n",
        "\n",
        "input_items = [item.strip().capitalize() for item in input_items]\n",
        "\n",
        "predict_next_item(input_items)\n"
      ],
      "metadata": {
        "id": "4Y0hTwXT28WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FP Growth\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "transactions = [\n",
        "    ['Milk', 'Bread', 'Butter'],\n",
        "    ['Bread', 'Butter'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Eggs'],\n",
        "    ['Milk', 'Bread', 'Diaper', 'Butter'],\n",
        "    ['Bread', 'Butter', 'Diaper'],\n",
        "    ['Milk', 'Bread', 'Diaper', 'Beer'],\n",
        "    ['Bread', 'Butter'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Cola'],\n",
        "    ['Milk', 'Bread', 'Butter'],\n",
        "    ['Bread', 'Diaper', 'Cola']\n",
        "]\n",
        "\n",
        "print(\"Transactions:\\n\", transactions)\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df = pd.DataFrame(te_array, columns=te.columns_)\n",
        "print(\"\\nBinary Matrix of Transactions:\\n\", df.head())\n",
        "\n",
        "frequent_itemsets = fpgrowth(df, min_support=0.3, use_colnames=True)\n",
        "\n",
        "print(\"\\nFrequent Itemsets:\\n\", frequent_itemsets)\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "print(\"\\nAssociation Rules:\\n\", rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "def predict_next_item(input_items):\n",
        "    input_items = set(input_items)\n",
        "\n",
        "    matched_rules = rules[rules['antecedents'].apply(lambda x: input_items.issubset(x))]\n",
        "\n",
        "    if matched_rules.empty:\n",
        "        print(\"\\nNo strong association found. Try with a different combination of items.\")\n",
        "        return None\n",
        "\n",
        "    best_rule = matched_rules.loc[matched_rules['confidence'].idxmax()]\n",
        "    predicted_item = list(best_rule['consequents'])[0]\n",
        "\n",
        "    print(f\"\\nBased on your input items {input_items}, you are likely to buy '{predicted_item}' with confidence of {best_rule['confidence']:.2f}\")\n",
        "    return predicted_item\n",
        "\n",
        "input_items = input(\"Enter the items you bought (comma separated): \").split(',')\n",
        "\n",
        "input_items = [item.strip().capitalize() for item in input_items]\n",
        "\n",
        "predict_next_item(input_items)\n"
      ],
      "metadata": {
        "id": "pIJr3JHS2-mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eclat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "transactions = [\n",
        "    ['Milk', 'Bread', 'Butter'],\n",
        "    ['Bread', 'Butter'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Eggs'],\n",
        "    ['Milk', 'Bread', 'Diaper', 'Butter'],\n",
        "    ['Bread', 'Butter', 'Diaper'],\n",
        "    ['Milk', 'Bread', 'Diaper', 'Beer'],\n",
        "    ['Bread', 'Butter'],\n",
        "    ['Milk', 'Diaper', 'Beer', 'Cola'],\n",
        "    ['Milk', 'Bread', 'Butter'],\n",
        "    ['Bread', 'Diaper', 'Cola']\n",
        "]\n",
        "\n",
        "print(\"Transactions:\\n\", transactions)\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_array = te.fit(transactions).transform(transactions)\n",
        "\n",
        "df = pd.DataFrame(te_array, columns=te.columns_)\n",
        "print(\"\\nBinary Matrix of Transactions:\\n\", df.head())\n",
        "\n",
        "def eclat(dataset, min_support=0.3):\n",
        "    itemsets = {}\n",
        "    num_transactions = len(dataset)\n",
        "\n",
        "    for item in dataset.columns:\n",
        "        support = np.sum(dataset[item]) / num_transactions\n",
        "        if support >= min_support:\n",
        "            itemsets[frozenset([item])] = support\n",
        "\n",
        "    items = list(itemsets.keys())\n",
        "    for i in range(len(items)):\n",
        "        for j in range(i + 1, len(items)):\n",
        "            combined_items = items[i] | items[j]\n",
        "            if combined_items not in itemsets:\n",
        "                combined_support = np.sum(dataset[list(combined_items)].all(axis=1)) / num_transactions\n",
        "                if combined_support >= min_support:\n",
        "                    itemsets[combined_items] = combined_support\n",
        "\n",
        "    itemsets_df = pd.DataFrame(list(itemsets.items()), columns=['itemsets', 'support'])\n",
        "    return itemsets_df\n",
        "\n",
        "frequent_itemsets = eclat(df, min_support=0.3)\n",
        "\n",
        "print(\"\\nFrequent Itemsets from ECLAT:\\n\", frequent_itemsets)\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "print(\"\\nAssociation Rules:\\n\", rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "def predict_next_item(input_items):\n",
        "    input_items = set(input_items)\n",
        "\n",
        "    matched_rules = rules[rules['antecedents'].apply(lambda x: input_items.issubset(x))]\n",
        "\n",
        "    if matched_rules.empty:\n",
        "        print(\"\\nNo strong association found. Try with a different combination of items.\")\n",
        "        return None\n",
        "\n",
        "    best_rule = matched_rules.loc[matched_rules['confidence'].idxmax()]\n",
        "    predicted_item = list(best_rule['consequents'])[0]\n",
        "\n",
        "    print(f\"\\nBased on your input items {input_items}, you are likely to buy '{predicted_item}' with confidence of {best_rule['confidence']:.2f}\")\n",
        "    return predicted_item\n",
        "\n",
        "input_items = input(\"Enter the items you bought (comma separated): \").split(',')\n",
        "\n",
        "input_items = [item.strip().capitalize() for item in input_items]\n",
        "\n",
        "predict_next_item(input_items)\n"
      ],
      "metadata": {
        "id": "6mIn1bOm3AIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}